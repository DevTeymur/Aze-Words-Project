{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3160aa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2936ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb82a277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20305 entries, 0 to 20304\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  20305 non-null  int64 \n",
      " 1   Text        20305 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 317.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f01821b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f16f784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>azərbaycan erməni ara son döyük harada ged ətr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>son məlumat azərbaycan kor infeksiya yeni yolu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>baş nazik müavin şahin Mustafayev gün tehran i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qış ev isti saxlama sadə ucuz üsulu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>iraq rəsmi xəbər agentlik irq türkmən paytaxt ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>azərbaycan baş nazir müavin şahin Mustafayev i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>min əhali olan mar şəhər i məktəb sayəsində zə...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>kor simptom haqqında getdikcə məlumat ək edil ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>türki hərbi təyyarə proqram çıxarıl ab yeni f ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>talib hakimiyyət gələn minlərlə əfqan qaçqın t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  azərbaycan erməni ara son döyük harada ged ətr...\n",
       "1  son məlumat azərbaycan kor infeksiya yeni yolu...\n",
       "2  baş nazik müavin şahin Mustafayev gün tehran i...\n",
       "3                qış ev isti saxlama sadə ucuz üsulu\n",
       "4  iraq rəsmi xəbər agentlik irq türkmən paytaxt ...\n",
       "5  azərbaycan baş nazir müavin şahin Mustafayev i...\n",
       "6  min əhali olan mar şəhər i məktəb sayəsində zə...\n",
       "7  kor simptom haqqında getdikcə məlumat ək edil ...\n",
       "8  türki hərbi təyyarə proqram çıxarıl ab yeni f ...\n",
       "9  talib hakimiyyət gələn minlərlə əfqan qaçqın t..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646dac84",
   "metadata": {},
   "source": [
    "# Categories:\n",
    "## First\n",
    "\n",
    "#### Positive:\n",
    "uğurlu, bilik, təhsil, incəsənət, bayram, festival, mübarək, apple pay, google pay, humanitar yardım, \n",
    "orden, medal, \n",
    "\n",
    "\n",
    "#### Negative:\n",
    "cinayət, polis, təəssüf, fövqəladə, yanğın, erməni, cəbhə, repressiya, mitinq, xəstəlik, nikol, cənazə\n",
    "\n",
    "\n",
    "#### Neutral:\n",
    "Paşa, yelo, abb, kapital bank, kart, iqtisadi, pul, metro\n",
    "\n",
    "## Second:\n",
    "#### Politics:\n",
    "milli məclis, hökumətlərarası, azərbaycan, qondarma, iqtisadi, mitinq, erməni, sülh, bəyannamə, prezident,\n",
    "\n",
    "\n",
    "#### Banking:\n",
    "yelo, bank, kredit, müştəri, apple\n",
    "\n",
    "#### Culture:\n",
    "mədəniyyət, şeir, poeziya, repressiya, sərgi, incəsənət, festival, sənətkar, bəstəkar, rəssam,\n",
    "\n",
    "\n",
    "#### Education:\n",
    "təhsil, tələbə, şagird, universitet, məktəb, əlifba\n",
    "\n",
    "#### Sport:\n",
    "\n",
    "#### Military:\n",
    "ordu, rəşadətli, orden, medal, \n",
    "\n",
    "#### Econimics\n",
    "iqtisadi, kart, bank, pul, manat, sərmayə\n",
    "\n",
    "#### Other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10ff3377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "insan söylə məşğul deyil insanın öncə vardır bəzən sıra gəlmə auster\n",
      "---------------------------------\n",
      "gün bak vaxtilə saat qət keçiril Düny kubok final oyunu keçiril hansı komandan qalib olma istər çempionat nəticə sayt bak bilək\n",
      "---------------------------------\n",
      "tut əlik i veriliş oktyabr az\n",
      "---------------------------------\n",
      "Füzul rayon polis şöbə əməkdaş rayon işğal azad edil ərazi keçir növbəti tədbir nəticə müxtəlif silah aşkar edil tapıl döyüş sursat götürül aidiyyət üzrə təhvil veril\n",
      "---------------------------------\n",
      "ton kreqq nəzər nöqtələ əsər müasir dünya incəsənət görkəmli nümayəndə biri məşhur britaniyalı heykəltəraş ton kreqq aid əsər nümayiş olun məkan sənətsevər xüsusi ab yara\n",
      "---------------------------------\n",
      "rt pa sərgi illik enerji nailiyyət izləmə qürur ulu öndər Heyd əl neft\n",
      "---------------------------------\n",
      "ilham Əliyev gələn türki gedəcək türki azərbaycan səfir cahid bağçı de i rt ərdo oktyabr qara noyabr i görük noyabr i türk şura dövlət başçı vii zirvə görük olacaq\n",
      "---------------------------------\n",
      "məzar başdaşı qoyun heykəl cüt ayaqqabı qoyun aya gey get vaqif səmə\n",
      "---------------------------------\n",
      "rusi xarici işlə nazik ser lavrov israili yeni xarici işlə nazik el cohe ara telefon danışıq ol\n",
      "---------------------------------\n",
      "proqram təminat üzrə mütəxəssis Ziy qaf biznes resurs planlaşdırılma proqram yaranma tarixi haqqında danış qon tam çıxış youtube kanal izlə spotif apple podcast vasitəsilə dinlə bilək\n"
     ]
    }
   ],
   "source": [
    "def returnRandom10Line(df = df):\n",
    "    random_numbers = [random.randint(0, df.shape[0]) for _ in range(10)]\n",
    "    for idx in random_numbers:\n",
    "        print(\"---------------------------------\")\n",
    "        print(df.loc[idx, 'Text'])\n",
    "\n",
    "returnRandom10Line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26bd7541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterRows(df = df):\n",
    "    df_copy = df.copy()\n",
    "    df_copy = df_copy[df_copy['Text'].str.split().str.len() > 5]\n",
    "    print(\"Number of droppend rows:\", df.shape[0] - df_copy.shape[0])\n",
    "    return df_copy.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06da1b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of droppend rows: 1770\n"
     ]
    }
   ],
   "source": [
    "res = filterRows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2b612c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = ['uğurlu', 'bilik', 'təhsil', 'incəsənət', 'bayram', 'festival', 'mübarək', 'apple pay', \n",
    "                  'google pay', 'humanitaryardım', 'orden', 'medal', 'bacarıq', 'macəraçı', 'mehriban', \n",
    "                  'razı', 'iddialı', 'mehriban', 'əyləncəli', 'cəsur', 'parlaq', 'sakit', 'bacarıqlı', \n",
    "                  'cazibədar', 'şən', 'ağıllı', 'mərhəmətli', 'inamlı', 'fikirli', 'cəsarətli', \n",
    "                  'nəzakətli', 'yaradıcı', 'etibarlı', 'cəsarətli', 'gözqamaşdırıcı', 'zərif', 'ləzzətli', \n",
    "                  'çalışqan', 'diplomatik', 'təmkinli', 'dinamik', 'istəkli', 'ciddi', 'yolagedən', \n",
    "                  'səmərəli', 'zərif', 'elit', 'empatik', 'enerjili', 'həvəsli', 'vacib', 'əla', \n",
    "                  'həyəcanlı', 'ekspert', 'qeyri-adi', 'ədalətli', 'sadiq', 'fantastik', 'qorxmaz', \n",
    "                  'yaxşı', 'gülməli', 'zərif', 'şanlı', 'yaxşı', 'mərhəmətli', 'xoşbəxt', 'harmonik', \n",
    "                  'faydalı', 'dürüst', 'hörmətli', 'ideal', 'inanılmaz', 'yenilikçi', 'ixtiraçı', \n",
    "                  'şən', 'mehriban', 'bilikli', 'lider', 'əfsanəvi', 'işıq', 'sadiq', 'şanslı', \n",
    "                  'möhtəşəm', 'möcüzə', 'yumşaq', 'motivasiyalı', 'təbii', 'gözəl', \n",
    "                  'optimist', 'görkəmli', 'zəhmətkeş', 'ehtiraslı', 'dinc', 'mükəmməl', 'səbirli', \n",
    "                  'davamlı', 'xeyriyyəçi', 'fəlsəfi', 'sakit', 'cəld', 'məşhur', 'kompaniya', 'endirim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc054af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_words = ['təsdiqləmək', 'şaqqal', 'xəbislik', 'xırtıldayan', 'uçurumlu', 'çürük', 'vəhşi', \n",
    "                  'mitinq', 'təəssüf', 'zəhlətökən', 'ümidsizlik', 'dəhşətli', 'iyrənc', 'cənazə', \n",
    "                  'bədbəxtlik', 'qorxu', 'sinizm', 'xaç', 'travma', 'xoşagəlməz', 'acı', 'rüsvayçılıq', \n",
    "                  'xəstəlik', 'qeyri-qənaətbəxş', 'qüsurlu', 'əhvalsız', 'pislik', 'günahkarlıq', 'terror', \n",
    "                  'kədər', 'depressiya', 'ən pis', 'utanmaq', 'qəzəbli', 'utandırıcı', 'dep efir', 'polis', \n",
    "                  'tənqid etmək', 'travmatik', 'səfalət', 'erməni', 'çarəsiz', 'yanğın', 'bədbəxt', \n",
    "                  'düşmənçilik', 'yazıq', 'cəbhə', 'xəstəlikverici', 'məyusedici', 'kədərləndirici', \n",
    "                  'üsyankar', 'narahat', 'nifrət', 'qıcıqlandırıcı', 'kədərli', 'güllük', 'qorxunc', \n",
    "                  'pis', 'narahatçılıq', 'fövqəladə', 'peşiman', 'günahkar', 'nikol', 'çirkin', \n",
    "                  'peşmanlıq', 'kinik', 'qınamaq', 'hücumedici', 'çarəsizlik', 'qıcıqlanan', 'incidici', \n",
    "                  'qorxulu', 'cinayət', 'repressiya', 'melanxolik', 'alçaq', 'narahatlıq', 'kinbaz', \n",
    "                  'etibarsızlıq', 'qeyri-adi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c80cc551",
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_words = ['Paşa', 'yelo', 'abb', 'kapital bank', 'kart', 'iqtisadi', 'pul', 'metro', 'diqqətli',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "305ae2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addClass(df = df):\n",
    "    data = df.copy()\n",
    "    final = pd.DataFrame(data = {'Text':[], 'tag': []})\n",
    "    data['tag'] = 'n/a'\n",
    "    print(f'Length of all data {data.shape[0]}')\n",
    "    \n",
    "    data['tag'] = data['Text'].apply(lambda x: any(x.find(f' {word} ')>=0 for word in positive_words))\n",
    "    data.loc[data['class'] == True, 'tag'] = 'positive'\n",
    "    positive_converted = data[data[\"tag\"] == 'positive'].shape[0]\n",
    "    positives = data[data[\"tag\"] == 'positive']\n",
    "    final = pd.concat([positives, final], axis = 0)\n",
    "    print(f'{positive_converted} converted, {data.shape[0] - positive_converted} remaining')\n",
    "    data = data[data['tag'] != 'positive']\n",
    "    \n",
    "    data['tag'] = data['Text'].apply(lambda x: any(x.find(f' {word} ')>=0 for word in negative_words))\n",
    "    \n",
    "    data.loc[data['class'] == True, 'tag'] = 'negative'\n",
    "    negative_converted = data[data[\"tag\"] == 'negative'].shape[0]\n",
    "    negatives = data[data[\"tag\"] == 'negative']\n",
    "    final = pd.concat([negatives, final], axis = 0)\n",
    "    print(f'{negative_converted} converted, {data.shape[0] - negative_converted} remaining')\n",
    "    data = data[data['tag'] != 'negative']\n",
    "    \n",
    "    data['tag'] = data['Text'].apply(lambda x: any(x.find(f' {word} ')>=0 for word in neutral_words))\n",
    "    data.loc[data['tag'] == True, 'tag'] == 'neutral'\n",
    "    neutral_converted = data[data[\"tag\"] == 'neutral'].shape[0]\n",
    "    negatives = data[data[\"tag\"] == 'neutral']\n",
    "    final = pd.concat([negatives, final], axis = 0)\n",
    "    print(f'{neutral_converted} converted, {data.shape[0] - neutral_converted} remaining')\n",
    "    data = data[data['tag'] != 'neutral']\n",
    "    \n",
    "    print(f'Length of data {data.shape[0]}')\n",
    "    print(f'Length of final {final.shape[0]}')\n",
    "    \n",
    "    return final.reset_index(drop=True), data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a282f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of all data 20305\n",
      "2978 converted, 17327 remaining\n",
      "1484 converted, 15843 remaining\n",
      "0 converted, 15843 remaining\n",
      "Length of data 15843\n",
      "Length of final 4462\n"
     ]
    }
   ],
   "source": [
    "final, data = addClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6d6696b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b hökümət ölkə xaric gəlmə vurulan vaksinlə siyahı genişləndir london yaşıl işıq ver vaksin siyahı noyabr dən etibarən sinov olacaq astra pfizer yoxsa sinov hansı yaxşı'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final[final['tag'] == 'positive']['Text'].iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb032a78",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m final\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18m__file__\u001b[39m),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/Analysis/result.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      2\u001b[0m              index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "final.to_csv(f'{os.path.abspath(os.path.join(os.path.dirname(__file__),\"..\"))}/Analysis/result.csv', \n",
    "             index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d59948e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m script_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;18;43m__file__\u001b[39;49m))\n\u001b[1;32m      2\u001b[0m parent_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(script_dir, os\u001b[38;5;241m.\u001b[39mpardir))\n\u001b[1;32m      3\u001b[0m analysis_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(parent_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnalysis\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "parent_dir = os.path.abspath(os.path.join(script_dir, os.pardir))\n",
    "analysis_dir = os.path.join(parent_dir, 'Analysis')\n",
    "if not os.path.exists(analysis_dir):\n",
    "    os.makedirs(analysis_dir)\n",
    "final.to_csv(os.path.join(analysis_dir, 'result.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2eded090",
   "metadata": {},
   "source": [
    "Have to divide the list with priority\n",
    "\n",
    "The order of defining class can be important\n",
    "\n",
    "Maybe not dropping the rows adding all classes to all rows to see\n",
    "\n",
    "https://monkeylearn.com/sentiment-analysis/#:~:text=Sentiment%20analysis%20(or%20opinion%20mining,feedback%2C%20and%20understand%20customer%20needs.\n",
    "\n",
    "Counts the number of positive and negative words that appear in a given text.\n",
    "If the number of positive word appearances is greater than the number of negative word appearances, the system returns a positive sentiment, and vice versa. If the numbers are even, the system will return a neutral sentiment."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7a594606",
   "metadata": {},
   "source": [
    "RNN Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d25df0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-07 18:12:46.295972: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-02-07 18:12:46.327173: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-07 18:12:46.327186: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'pad_sequences' from 'keras.preprocessing.sequence' (/home/ufaz/.local/lib/python3.8/site-packages/keras/preprocessing/sequence.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Embedding, LSTM, Dense\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'pad_sequences' from 'keras.preprocessing.sequence' (/home/ufaz/.local/lib/python3.8/site-packages/keras/preprocessing/sequence.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Tokenize the sentences\n",
    "texts = ['This is a positive sentence', 'This is a negative sentence', 'A neutral sentence']\n",
    "tokenized_texts = [text.split() for text in texts]\n",
    "\n",
    "# Determine the maximum length of the texts\n",
    "max_len = max([len(text) for text in tokenized_texts])\n",
    "\n",
    "# Pad the texts so that they all have the same length\n",
    "padded_texts = pad_sequences(tokenized_texts, maxlen=max_len, padding='post')\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=max_len))\n",
    "model.add(LSTM(units=32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded_texts, labels, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431506cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
