{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-02-28T09:01:42.496685Z",
     "iopub.status.busy": "2023-02-28T09:01:42.495957Z",
     "iopub.status.idle": "2023-02-28T09:01:42.501524Z",
     "shell.execute_reply": "2023-02-28T09:01:42.500080Z",
     "shell.execute_reply.started": "2023-02-28T09:01:42.496645Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T09:01:43.405640Z",
     "iopub.status.busy": "2023-02-28T09:01:43.405139Z",
     "iopub.status.idle": "2023-02-28T09:01:43.431392Z",
     "shell.execute_reply": "2023-02-28T09:01:43.430386Z",
     "shell.execute_reply.started": "2023-02-28T09:01:43.405596Z"
    }
   },
   "outputs": [],
   "source": [
    "file = open('/kaggle/input/main-twitter/vocabulary.pkl', \"rb\")\n",
    "vocabulary = pkl.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T09:01:43.664947Z",
     "iopub.status.busy": "2023-02-28T09:01:43.664586Z",
     "iopub.status.idle": "2023-02-28T09:01:43.738111Z",
     "shell.execute_reply": "2023-02-28T09:01:43.737144Z",
     "shell.execute_reply.started": "2023-02-28T09:01:43.664915Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/main-twitter/main.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T09:01:43.941345Z",
     "iopub.status.busy": "2023-02-28T09:01:43.940355Z",
     "iopub.status.idle": "2023-02-28T09:01:43.957306Z",
     "shell.execute_reply": "2023-02-28T09:01:43.956270Z",
     "shell.execute_reply.started": "2023-02-28T09:01:43.941293Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df[df['class'] != '-']\n",
    "df.drop(['name'], axis=1, inplace=True)\n",
    "df.dropna(how='any', inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df['class'] = df['class'].astype(int)\n",
    "df = df[df['class'] >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T09:01:44.250651Z",
     "iopub.status.busy": "2023-02-28T09:01:44.249885Z",
     "iopub.status.idle": "2023-02-28T09:01:44.261549Z",
     "shell.execute_reply": "2023-02-28T09:01:44.260158Z",
     "shell.execute_reply.started": "2023-02-28T09:01:44.250614Z"
    }
   },
   "outputs": [],
   "source": [
    "def isInVocabulary(word, voc=vocabulary):\n",
    "    \"\"\"\n",
    "    Searchs for word from dictonary based on first letter\n",
    "    \"\"\"\n",
    "    first_letter = word[0]\n",
    "    if first_letter not in voc:\n",
    "        return False\n",
    "    if word in voc[first_letter]:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T09:01:44.852924Z",
     "iopub.status.busy": "2023-02-28T09:01:44.852015Z",
     "iopub.status.idle": "2023-02-28T09:01:44.864793Z",
     "shell.execute_reply": "2023-02-28T09:01:44.863754Z",
     "shell.execute_reply.started": "2023-02-28T09:01:44.852876Z"
    }
   },
   "outputs": [],
   "source": [
    "def stem(string):\n",
    "    l=[]\n",
    "    vowels=[\"a\",\"ı\",\"o\",\"u\",\"e\",\"ə\",\"i\"]\n",
    "    string=string.split()\n",
    "    for i in string:\n",
    "        if i.isupper() or (string.index(i)!=0 and i[0].isupper()):\n",
    "            # xüsusi isimlər və abbr. üçün\n",
    "            l.append(i)\n",
    "        else:\n",
    "            for j in range(len(i),0,-1):\n",
    "                if isInVocabulary(i[:j].casefold()): # i[:j].casefold() in words:\n",
    "                    # adi şəkilçilər üçün\n",
    "                    l.append(i[:j])\n",
    "                    break\n",
    "                elif i[j-1] in vowels and (i[j-2]==\"y\" or i[j-2]==\"ğ\") :\n",
    "                    # bitişdirici samitlər üçün\n",
    "                    if isInVocabulary((i[:j-2]+\"k\").casefold()): # (i[:j-2]+\"k\").casefold() in words:\n",
    "                        l.append(i[:j-2]+\"k\")\n",
    "                        break\n",
    "                    elif isInVocabulary((i[:j-2]+\"q\").casefold()): # (i[:j-2]+\"q\").casefold() in words:\n",
    "                        l.append(i[:j-2]+\"q\")\n",
    "                        break\n",
    "    return ' '.join(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T09:01:45.332915Z",
     "iopub.status.busy": "2023-02-28T09:01:45.332248Z",
     "iopub.status.idle": "2023-02-28T09:02:00.951283Z",
     "shell.execute_reply": "2023-02-28T09:02:00.950230Z",
     "shell.execute_reply.started": "2023-02-28T09:01:45.332878Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].apply(lambda row: stem(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T09:02:00.953671Z",
     "iopub.status.busy": "2023-02-28T09:02:00.953322Z",
     "iopub.status.idle": "2023-02-28T09:02:00.983461Z",
     "shell.execute_reply": "2023-02-28T09:02:00.982556Z",
     "shell.execute_reply.started": "2023-02-28T09:02:00.953642Z"
    }
   },
   "outputs": [],
   "source": [
    "stemmed_data = df.copy()\n",
    "stemmed_data.to_csv('clean_stemmed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-02-28T09:02:00.985449Z",
     "iopub.status.busy": "2023-02-28T09:02:00.985040Z",
     "iopub.status.idle": "2023-02-28T09:02:10.612719Z",
     "shell.execute_reply": "2023-02-28T09:02:10.611462Z",
     "shell.execute_reply.started": "2023-02-28T09:02:00.985392Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lazy-text-predict in /opt/conda/lib/python3.7/site-packages (0.0.11)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from lazy-text-predict) (1.0.2)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from lazy-text-predict) (1.13.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (from lazy-text-predict) (4.26.1)\n",
      "Requirement already satisfied: nlp in /opt/conda/lib/python3.7/site-packages (from lazy-text-predict) (0.4.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from lazy-text-predict) (0.1.97)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from lazy-text-predict) (1.21.6)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from nlp->lazy-text-predict) (1.3.5)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from nlp->lazy-text-predict) (3.2.0)\n",
      "Requirement already satisfied: pyarrow>=0.16.0 in /opt/conda/lib/python3.7/site-packages (from nlp->lazy-text-predict) (5.0.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from nlp->lazy-text-predict) (4.64.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from nlp->lazy-text-predict) (3.9.0)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from nlp->lazy-text-predict) (0.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from nlp->lazy-text-predict) (2.28.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->lazy-text-predict) (1.7.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->lazy-text-predict) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->lazy-text-predict) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->lazy-text-predict) (4.4.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers->lazy-text-predict) (0.13.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers->lazy-text-predict) (2021.11.10)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers->lazy-text-predict) (23.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers->lazy-text-predict) (4.11.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers->lazy-text-predict) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.7/site-packages (from transformers->lazy-text-predict) (0.12.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->nlp->lazy-text-predict) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->nlp->lazy-text-predict) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->nlp->lazy-text-predict) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->nlp->lazy-text-predict) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers->lazy-text-predict) (3.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->nlp->lazy-text-predict) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->nlp->lazy-text-predict) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->nlp->lazy-text-predict) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install lazy-text-predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T09:02:10.616216Z",
     "iopub.status.busy": "2023-02-28T09:02:10.615792Z",
     "iopub.status.idle": "2023-02-28T09:02:10.622492Z",
     "shell.execute_reply": "2023-02-28T09:02:10.621449Z",
     "shell.execute_reply.started": "2023-02-28T09:02:10.616154Z"
    }
   },
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T09:02:10.625088Z",
     "iopub.status.busy": "2023-02-28T09:02:10.623964Z",
     "iopub.status.idle": "2023-02-28T10:36:39.015333Z",
     "shell.execute_reply": "2023-02-28T10:36:39.014436Z",
     "shell.execute_reply.started": "2023-02-28T09:02:10.625052Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train length: 5760\n",
      "X_test length: 641\n",
      "Y_train length: 5760\n",
      "Y_test length: 641\n",
      "Training on a dataset with 3 labels\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea3157465cd4b3e9ecc33493d0f3f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f29a074e76431185966ade30096da2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d55d02296f427d984fdb3396937c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf5c3e84e9a944aa9345f62bab3f6403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be92208207f74926be21a09374a1b7f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb63aee04e6f4dfaaf0f0e8f1a895750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632b37b0ece548a6bc7ef34aaa9c9182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 5760\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1800\n",
      "  Number of trainable parameters = 109484547\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20230228_091646-e3uui1xr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mytestmailx601/huggingface/runs/e3uui1xr' target=\"_blank\">vital-grass-6</a></strong> to <a href='https://wandb.ai/mytestmailx601/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mytestmailx601/huggingface' target=\"_blank\">https://wandb.ai/mytestmailx601/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mytestmailx601/huggingface/runs/e3uui1xr' target=\"_blank\">https://wandb.ai/mytestmailx601/huggingface/runs/e3uui1xr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1800' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1800/1800 25:28, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.003600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.725900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.460500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/bert-base-uncased/checkpoint-500\n",
      "Configuration saved in ./results/bert-base-uncased/checkpoint-500/config.json\n",
      "Model weights saved in ./results/bert-base-uncased/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to ./results/bert-base-uncased/checkpoint-1000\n",
      "Configuration saved in ./results/bert-base-uncased/checkpoint-1000/config.json\n",
      "Model weights saved in ./results/bert-base-uncased/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to ./results/bert-base-uncased/checkpoint-1500\n",
      "Configuration saved in ./results/bert-base-uncased/checkpoint-1500/config.json\n",
      "Model weights saved in ./results/bert-base-uncased/checkpoint-1500/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 641\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11/11 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'0': {'precision': 0.6761904761904762, 'recall': 0.6396396396396397, 'f1-score': 0.6574074074074076, 'support': 111}, '1': {'precision': 0.7483443708609272, 'recall': 0.5824742268041238, 'f1-score': 0.655072463768116, 'support': 194}, '2': {'precision': 0.7324675324675325, 'recall': 0.8392857142857143, 'f1-score': 0.782246879334258, 'support': 336}, 'accuracy': 0.7269890795631825, 'macro avg': {'precision': 0.7190007931729786, 'recall': 0.6871331935764925, 'f1-score': 0.6982422501699271, 'support': 641}, 'weighted avg': {'precision': 0.7275273661673225, 'recall': 0.7269890795631825, 'f1-score': 0.7221392069415716, 'support': 641}}\" of type <class 'dict'> for key \"eval/full_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to bert-base-uncased_model\n",
      "Configuration saved in bert-base-uncased_model/config.json\n",
      "Model weights saved in bert-base-uncased_model/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8534520268440247, 'eval_accuracy': 0.7269890795631825, 'eval_f1': 0.7221392069415716, 'eval_precision': 0.7275273661673225, 'eval_recall': 0.7269890795631825, 'eval_full_report': {'0': {'precision': 0.6761904761904762, 'recall': 0.6396396396396397, 'f1-score': 0.6574074074074076, 'support': 111}, '1': {'precision': 0.7483443708609272, 'recall': 0.5824742268041238, 'f1-score': 0.655072463768116, 'support': 194}, '2': {'precision': 0.7324675324675325, 'recall': 0.8392857142857143, 'f1-score': 0.782246879334258, 'support': 336}, 'accuracy': 0.7269890795631825, 'macro avg': {'precision': 0.7190007931729786, 'recall': 0.6871331935764925, 'f1-score': 0.6982422501699271, 'support': 641}, 'weighted avg': {'precision': 0.7275273661673225, 'recall': 0.7269890795631825, 'f1-score': 0.7221392069415716, 'support': 641}}, 'eval_runtime': 11.0167, 'eval_samples_per_second': 58.184, 'eval_steps_per_second': 0.998, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on a dataset with 3 labels\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13fa2895d109477b8dd93a9b7ffaf7bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spiece.model from cache at /root/.cache/huggingface/hub/models--albert-base-v2/snapshots/51dbd9db43a0c6eba97f74b91ce26fface509e0b/spiece.model\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75620aca1b3e4f53bb9a7ac90a385a6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--albert-base-v2/snapshots/51dbd9db43a0c6eba97f74b91ce26fface509e0b/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"albert-base-v2\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--albert-base-v2/snapshots/51dbd9db43a0c6eba97f74b91ce26fface509e0b/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "349dbfc1b64a47789b8f894502be5477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/47.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--albert-base-v2/snapshots/51dbd9db43a0c6eba97f74b91ce26fface509e0b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c818c9c9f96f47438ed407445754e0ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1486e922bbb4861a5738a451c48fc9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 5760\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1800\n",
      "  Number of trainable parameters = 11685891\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1800' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1800/1800 26:35, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.024300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.018700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.012900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/albert-base-v2/checkpoint-500\n",
      "Configuration saved in ./results/albert-base-v2/checkpoint-500/config.json\n",
      "Model weights saved in ./results/albert-base-v2/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to ./results/albert-base-v2/checkpoint-1000\n",
      "Configuration saved in ./results/albert-base-v2/checkpoint-1000/config.json\n",
      "Model weights saved in ./results/albert-base-v2/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to ./results/albert-base-v2/checkpoint-1500\n",
      "Configuration saved in ./results/albert-base-v2/checkpoint-1500/config.json\n",
      "Model weights saved in ./results/albert-base-v2/checkpoint-1500/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 641\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11/11 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'0': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 111}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 194}, '2': {'precision': 0.5241809672386896, 'recall': 1.0, 'f1-score': 0.6878198567041965, 'support': 336}, 'accuracy': 0.5241809672386896, 'macro avg': {'precision': 0.1747269890795632, 'recall': 0.3333333333333333, 'f1-score': 0.2292732855680655, 'support': 641}, 'weighted avg': {'precision': 0.27476568641528815, 'recall': 0.5241809672386896, 'f1-score': 0.3605420777731826, 'support': 641}}\" of type <class 'dict'> for key \"eval/full_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to albert-base-v2_model\n",
      "Configuration saved in albert-base-v2_model/config.json\n",
      "Model weights saved in albert-base-v2_model/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9678168296813965, 'eval_accuracy': 0.5241809672386896, 'eval_f1': 0.3605420777731826, 'eval_precision': 0.27476568641528815, 'eval_recall': 0.5241809672386896, 'eval_full_report': {'0': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 111}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 194}, '2': {'precision': 0.5241809672386896, 'recall': 1.0, 'f1-score': 0.6878198567041965, 'support': 336}, 'accuracy': 0.5241809672386896, 'macro avg': {'precision': 0.1747269890795632, 'recall': 0.3333333333333333, 'f1-score': 0.2292732855680655, 'support': 641}, 'weighted avg': {'precision': 0.27476568641528815, 'recall': 0.5241809672386896, 'f1-score': 0.3605420777731826, 'support': 641}}, 'eval_runtime': 12.3947, 'eval_samples_per_second': 51.716, 'eval_steps_per_second': 0.887, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on a dataset with 3 labels\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6746662f14bb46389a2388a2af87754b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9880e72ef664fdc8f97cc3638faadb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/vocab.json\n",
      "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98171923b2b54195a08bb0576bc2d0fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3163b31f879443ec9132f6d18b84741e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/pytorch_model.bin\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f60c04f488424b69b548c9baf7db4ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "923e84a19d52450e9cf63428c5242d52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 5760\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1800\n",
      "  Number of trainable parameters = 124647939\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1800' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1800/1800 25:35, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.973500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.786800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.602200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/roberta-base/checkpoint-500\n",
      "Configuration saved in ./results/roberta-base/checkpoint-500/config.json\n",
      "Model weights saved in ./results/roberta-base/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to ./results/roberta-base/checkpoint-1000\n",
      "Configuration saved in ./results/roberta-base/checkpoint-1000/config.json\n",
      "Model weights saved in ./results/roberta-base/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to ./results/roberta-base/checkpoint-1500\n",
      "Configuration saved in ./results/roberta-base/checkpoint-1500/config.json\n",
      "Model weights saved in ./results/roberta-base/checkpoint-1500/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 641\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11/11 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'0': {'precision': 0.5652173913043478, 'recall': 0.5855855855855856, 'f1-score': 0.575221238938053, 'support': 111}, '1': {'precision': 0.7125748502994012, 'recall': 0.6134020618556701, 'f1-score': 0.6592797783933518, 'support': 194}, '2': {'precision': 0.7409470752089137, 'recall': 0.7916666666666666, 'f1-score': 0.7654676258992805, 'support': 336}, 'accuracy': 0.7020280811232449, 'macro avg': {'precision': 0.6729131056042208, 'recall': 0.6635514380359742, 'f1-score': 0.6666562144102285, 'support': 641}, 'weighted avg': {'precision': 0.7019295922980677, 'recall': 0.7020280811232449, 'f1-score': 0.7003852680695669, 'support': 641}}\" of type <class 'dict'> for key \"eval/full_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to roberta-base_model\n",
      "Configuration saved in roberta-base_model/config.json\n",
      "Model weights saved in roberta-base_model/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7989315390586853, 'eval_accuracy': 0.7020280811232449, 'eval_f1': 0.7003852680695669, 'eval_precision': 0.7019295922980677, 'eval_recall': 0.7020280811232449, 'eval_full_report': {'0': {'precision': 0.5652173913043478, 'recall': 0.5855855855855856, 'f1-score': 0.575221238938053, 'support': 111}, '1': {'precision': 0.7125748502994012, 'recall': 0.6134020618556701, 'f1-score': 0.6592797783933518, 'support': 194}, '2': {'precision': 0.7409470752089137, 'recall': 0.7916666666666666, 'f1-score': 0.7654676258992805, 'support': 336}, 'accuracy': 0.7020280811232449, 'macro avg': {'precision': 0.6729131056042208, 'recall': 0.6635514380359742, 'f1-score': 0.6666562144102285, 'support': 641}, 'weighted avg': {'precision': 0.7019295922980677, 'recall': 0.7020280811232449, 'f1-score': 0.7003852680695669, 'support': 641}}, 'eval_runtime': 10.9735, 'eval_samples_per_second': 58.414, 'eval_steps_per_second': 1.002, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on a dataset with 3 labels\n",
      "ERROR\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "best parameters are:\n",
      "{'clf__alpha': 0.001, 'clf__penalty': 'l1', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}\n",
      "{'eval_loss': 0.36349453978159124, 'eval_accuracy': 0.6365054602184087, 'eval_f1': 0.5136461850294133, 'eval_precision': 0.7675142799902877, 'eval_recall': 0.4987157628910206, 'eval_full_report': '              precision    recall  f1-score   support\\n\\n           0       0.95      0.18      0.30       111\\n           1       0.75      0.38      0.51       194\\n           2       0.60      0.93      0.73       336\\n\\n    accuracy                           0.64       641\\n   macro avg       0.77      0.50      0.51       641\\nweighted avg       0.71      0.64      0.59       641\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on a dataset with 3 labels\n",
      "ERROR\n",
      "best parameters are:\n",
      "{'clf__alpha': 0.1, 'clf__fit_prior': True, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}\n",
      "{'eval_loss': 0.24648985959438377, 'eval_accuracy': 0.7535101404056163, 'eval_f1': 0.7198045374146003, 'eval_precision': 0.7703727928854219, 'eval_recall': 0.6927701818165737, 'eval_full_report': '              precision    recall  f1-score   support\\n\\n           0       0.77      0.54      0.63       111\\n           1       0.82      0.66      0.73       194\\n           2       0.73      0.88      0.80       336\\n\\n    accuracy                           0.75       641\\n   macro avg       0.77      0.69      0.72       641\\nweighted avg       0.76      0.75      0.75       641\\n'}\n",
      "                    Model            loss        accuracy              f1       precision          recall\n",
      "        bert-base-uncased         0.85345         0.72699         0.72214         0.72753         0.72699\n",
      "           albert-base-v2         0.96782         0.52418         0.36054         0.27477         0.52418\n",
      "             roberta-base         0.79893         0.70203         0.70039         0.70193         0.70203\n",
      "               linear_SVM         0.36349         0.63651         0.51365         0.76751         0.49872\n",
      "multinomial_naive_bayesian         0.24649         0.75351          0.7198         0.77037         0.69277\n"
     ]
    }
   ],
   "source": [
    "from lazytextpredict import basic_classification\n",
    "\n",
    "trial=basic_classification.LTP(Xdata=list(df[\"Text\"].values),Ydata=list(df[\"class\"].values), x_col='Text', y_col='class', models='all') \n",
    "# Xdata is a list of text entries, and Ydata is a list of corresponding labels.\n",
    "# csv and xlsx give options to load data from those file formats (you can pass the file or the file's location)\n",
    "# x_col and y_col are strings that specify the columns of the # text and label columns in your csv or xlsx file respectively.\n",
    "# You can choose between 'transformers'-based, 'count-vectorizer'-based, and 'all' models.\n",
    "\n",
    "trial.run(training_epochs=5) \n",
    "#This trains the models specified above on the data you loaded. \n",
    "#Here you can specify the number of training epochs. \n",
    "#Fewer training epochs will give poorer performance, but will run quicker to allow debugging.\n",
    "\n",
    "trial.print_metrics_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
