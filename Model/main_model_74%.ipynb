{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-03-05T08:11:16.974561Z",
     "iopub.status.busy": "2023-03-05T08:11:16.973937Z",
     "iopub.status.idle": "2023-03-05T08:11:16.997777Z",
     "shell.execute_reply": "2023-03-05T08:11:16.996804Z",
     "shell.execute_reply.started": "2023-03-05T08:11:16.974509Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T08:11:27.676866Z",
     "iopub.status.busy": "2023-03-05T08:11:27.675867Z",
     "iopub.status.idle": "2023-03-05T08:11:27.718782Z",
     "shell.execute_reply": "2023-03-05T08:11:27.717723Z",
     "shell.execute_reply.started": "2023-03-05T08:11:27.676816Z"
    }
   },
   "outputs": [],
   "source": [
    "file = open('/kaggle/input/main-twitter/vocabulary.pkl', \"rb\")\n",
    "vocabulary = pkl.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T08:11:28.508798Z",
     "iopub.status.busy": "2023-03-05T08:11:28.507949Z",
     "iopub.status.idle": "2023-03-05T08:11:28.644665Z",
     "shell.execute_reply": "2023-03-05T08:11:28.643593Z",
     "shell.execute_reply.started": "2023-03-05T08:11:28.508761Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/main-twitter/labeled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T08:11:30.149732Z",
     "iopub.status.busy": "2023-03-05T08:11:30.149060Z",
     "iopub.status.idle": "2023-03-05T08:11:30.176986Z",
     "shell.execute_reply": "2023-03-05T08:11:30.175991Z",
     "shell.execute_reply.started": "2023-03-05T08:11:30.149696Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df[df['class'] != '-']\n",
    "df.drop(['name'], axis=1, inplace=True)\n",
    "df.dropna(how='any', inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df['class'] = df['class'].astype(int)\n",
    "df = df[df['class'] >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T08:11:32.064513Z",
     "iopub.status.busy": "2023-03-05T08:11:32.063799Z",
     "iopub.status.idle": "2023-03-05T08:11:32.070989Z",
     "shell.execute_reply": "2023-03-05T08:11:32.069740Z",
     "shell.execute_reply.started": "2023-03-05T08:11:32.064476Z"
    }
   },
   "outputs": [],
   "source": [
    "def isInVocabulary(word, voc=vocabulary):\n",
    "    \"\"\"\n",
    "    Searchs for word from dictonary based on first letter\n",
    "    \"\"\"\n",
    "    first_letter = word[0]\n",
    "    if first_letter not in voc:\n",
    "        return False\n",
    "    if word in voc[first_letter]:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T08:11:33.749883Z",
     "iopub.status.busy": "2023-03-05T08:11:33.749098Z",
     "iopub.status.idle": "2023-03-05T08:11:33.758659Z",
     "shell.execute_reply": "2023-03-05T08:11:33.757577Z",
     "shell.execute_reply.started": "2023-03-05T08:11:33.749844Z"
    }
   },
   "outputs": [],
   "source": [
    "def stem(string):\n",
    "    l=[]\n",
    "    vowels=[\"a\",\"ı\",\"o\",\"u\",\"e\",\"ə\",\"i\"]\n",
    "    string=string.split()\n",
    "    for i in string:\n",
    "        if i.isupper() or (string.index(i)!=0 and i[0].isupper()):\n",
    "            # xüsusi isimlər və abbr. üçün\n",
    "            l.append(i)\n",
    "        else:\n",
    "            for j in range(len(i),0,-1):\n",
    "                if isInVocabulary(i[:j].casefold()): # i[:j].casefold() in words:\n",
    "                    # adi şəkilçilər üçün\n",
    "                    l.append(i[:j])\n",
    "                    break\n",
    "                elif i[j-1] in vowels and (i[j-2]==\"y\" or i[j-2]==\"ğ\") :\n",
    "                    # bitişdirici samitlər üçün\n",
    "                    if isInVocabulary((i[:j-2]+\"k\").casefold()): # (i[:j-2]+\"k\").casefold() in words:\n",
    "                        l.append(i[:j-2]+\"k\")\n",
    "                        break\n",
    "                    elif isInVocabulary((i[:j-2]+\"q\").casefold()): # (i[:j-2]+\"q\").casefold() in words:\n",
    "                        l.append(i[:j-2]+\"q\")\n",
    "                        break\n",
    "    return ' '.join(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T08:11:34.623923Z",
     "iopub.status.busy": "2023-03-05T08:11:34.623553Z",
     "iopub.status.idle": "2023-03-05T08:11:59.819209Z",
     "shell.execute_reply": "2023-03-05T08:11:59.818004Z",
     "shell.execute_reply.started": "2023-03-05T08:11:34.623891Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].apply(lambda row: stem(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T08:12:04.490283Z",
     "iopub.status.busy": "2023-03-05T08:12:04.489834Z",
     "iopub.status.idle": "2023-03-05T08:12:04.544706Z",
     "shell.execute_reply": "2023-03-05T08:12:04.543623Z",
     "shell.execute_reply.started": "2023-03-05T08:12:04.490239Z"
    }
   },
   "outputs": [],
   "source": [
    "stemmed_data = df.copy()\n",
    "stemmed_data.to_csv('clean_stemmed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-03-05T08:12:16.803748Z",
     "iopub.status.busy": "2023-03-05T08:12:16.803263Z",
     "iopub.status.idle": "2023-03-05T08:12:30.469791Z",
     "shell.execute_reply": "2023-03-05T08:12:30.468555Z",
     "shell.execute_reply.started": "2023-03-05T08:12:16.803711Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lazy-text-predict\n",
      "  Downloading lazy_text_predict-0.0.11-py3-none-any.whl (8.7 kB)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (from lazy-text-predict) (4.26.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from lazy-text-predict) (1.0.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from lazy-text-predict) (1.21.6)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from lazy-text-predict) (0.1.97)\n",
      "Collecting nlp\n",
      "  Downloading nlp-0.4.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from lazy-text-predict) (1.13.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from nlp->lazy-text-predict) (1.3.5)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from nlp->lazy-text-predict) (0.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from nlp->lazy-text-predict) (2.28.2)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from nlp->lazy-text-predict) (3.2.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from nlp->lazy-text-predict) (3.9.0)\n",
      "Requirement already satisfied: pyarrow>=0.16.0 in /opt/conda/lib/python3.7/site-packages (from nlp->lazy-text-predict) (5.0.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from nlp->lazy-text-predict) (4.64.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->lazy-text-predict) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->lazy-text-predict) (1.7.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->lazy-text-predict) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->lazy-text-predict) (4.4.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.7/site-packages (from transformers->lazy-text-predict) (0.12.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers->lazy-text-predict) (4.11.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers->lazy-text-predict) (2021.11.10)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers->lazy-text-predict) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers->lazy-text-predict) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers->lazy-text-predict) (0.13.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->nlp->lazy-text-predict) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->nlp->lazy-text-predict) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->nlp->lazy-text-predict) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->nlp->lazy-text-predict) (3.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers->lazy-text-predict) (3.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->nlp->lazy-text-predict) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->nlp->lazy-text-predict) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->nlp->lazy-text-predict) (1.16.0)\n",
      "Installing collected packages: nlp, lazy-text-predict\n",
      "Successfully installed lazy-text-predict-0.0.11 nlp-0.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install lazy-text-predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T08:12:10.591336Z",
     "iopub.status.busy": "2023-03-05T08:12:10.590607Z",
     "iopub.status.idle": "2023-03-05T08:12:10.596126Z",
     "shell.execute_reply": "2023-03-05T08:12:10.595034Z",
     "shell.execute_reply.started": "2023-03-05T08:12:10.591300Z"
    }
   },
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T08:12:47.933317Z",
     "iopub.status.busy": "2023-03-05T08:12:47.932471Z",
     "iopub.status.idle": "2023-03-05T10:16:00.672086Z",
     "shell.execute_reply": "2023-03-05T10:16:00.671096Z",
     "shell.execute_reply.started": "2023-03-05T08:12:47.933278Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train length: 8812\n",
      "X_test length: 980\n",
      "Y_train length: 8812\n",
      "Y_test length: 980\n",
      "Training on a dataset with 3 labels\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b261348713e4005b0a421affcfc6e08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9afe1cd29882414aa6106016febb1b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ad25f31b16b419c84caa2e974754383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89334d9905fb44eb9c30d05928e664f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de78e51fdc6f4f2d8663746ef22793b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abcf8168e3eb444fa04a8e619158672c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b659c15fc4cc4c3891f30a6cc97fc756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 8812\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2755\n",
      "  Number of trainable parameters = 109484547\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20230305_081355-emjqnysr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mytestmailx601/huggingface/runs/emjqnysr' target=\"_blank\">proud-dream-7</a></strong> to <a href='https://wandb.ai/mytestmailx601/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mytestmailx601/huggingface' target=\"_blank\">https://wandb.ai/mytestmailx601/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mytestmailx601/huggingface/runs/emjqnysr' target=\"_blank\">https://wandb.ai/mytestmailx601/huggingface/runs/emjqnysr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2755' max='2755' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2755/2755 39:02, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.958000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.772200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.601200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.464300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.332100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/bert-base-uncased/checkpoint-500\n",
      "Configuration saved in ./results/bert-base-uncased/checkpoint-500/config.json\n",
      "Model weights saved in ./results/bert-base-uncased/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to ./results/bert-base-uncased/checkpoint-1000\n",
      "Configuration saved in ./results/bert-base-uncased/checkpoint-1000/config.json\n",
      "Model weights saved in ./results/bert-base-uncased/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to ./results/bert-base-uncased/checkpoint-1500\n",
      "Configuration saved in ./results/bert-base-uncased/checkpoint-1500/config.json\n",
      "Model weights saved in ./results/bert-base-uncased/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to ./results/bert-base-uncased/checkpoint-2000\n",
      "Configuration saved in ./results/bert-base-uncased/checkpoint-2000/config.json\n",
      "Model weights saved in ./results/bert-base-uncased/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to ./results/bert-base-uncased/checkpoint-2500\n",
      "Configuration saved in ./results/bert-base-uncased/checkpoint-2500/config.json\n",
      "Model weights saved in ./results/bert-base-uncased/checkpoint-2500/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 980\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'0': {'precision': 0.6845238095238095, 'recall': 0.6216216216216216, 'f1-score': 0.6515580736543909, 'support': 185}, '1': {'precision': 0.6853146853146853, 'recall': 0.6805555555555556, 'f1-score': 0.6829268292682927, 'support': 288}, '2': {'precision': 0.747148288973384, 'recall': 0.7751479289940828, 'f1-score': 0.7608906098741529, 'support': 507}, 'accuracy': 0.7183673469387755, 'macro avg': {'precision': 0.7056622612706263, 'recall': 0.6924417020570868, 'f1-score': 0.6984585042656121, 'support': 980}, 'weighted avg': {'precision': 0.7171548129000407, 'recall': 0.7183673469387755, 'f1-score': 0.7173394996546184, 'support': 980}}\" of type <class 'dict'> for key \"eval/full_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to bert-base-uncased_model\n",
      "Configuration saved in bert-base-uncased_model/config.json\n",
      "Model weights saved in bert-base-uncased_model/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8994652032852173, 'eval_accuracy': 0.7183673469387755, 'eval_f1': 0.7173394996546184, 'eval_precision': 0.7171548129000407, 'eval_recall': 0.7183673469387755, 'eval_full_report': {'0': {'precision': 0.6845238095238095, 'recall': 0.6216216216216216, 'f1-score': 0.6515580736543909, 'support': 185}, '1': {'precision': 0.6853146853146853, 'recall': 0.6805555555555556, 'f1-score': 0.6829268292682927, 'support': 288}, '2': {'precision': 0.747148288973384, 'recall': 0.7751479289940828, 'f1-score': 0.7608906098741529, 'support': 507}, 'accuracy': 0.7183673469387755, 'macro avg': {'precision': 0.7056622612706263, 'recall': 0.6924417020570868, 'f1-score': 0.6984585042656121, 'support': 980}, 'weighted avg': {'precision': 0.7171548129000407, 'recall': 0.7183673469387755, 'f1-score': 0.7173394996546184, 'support': 980}}, 'eval_runtime': 16.7731, 'eval_samples_per_second': 58.427, 'eval_steps_per_second': 0.954, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on a dataset with 3 labels\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "012c6659021947c2b9cad02428aa8f2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spiece.model from cache at /root/.cache/huggingface/hub/models--albert-base-v2/snapshots/51dbd9db43a0c6eba97f74b91ce26fface509e0b/spiece.model\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "779859b3c81e468ba5ba7a73527bee17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--albert-base-v2/snapshots/51dbd9db43a0c6eba97f74b91ce26fface509e0b/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"albert-base-v2\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--albert-base-v2/snapshots/51dbd9db43a0c6eba97f74b91ce26fface509e0b/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc335081c8f40d99bf1d3df03ff2e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/47.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--albert-base-v2/snapshots/51dbd9db43a0c6eba97f74b91ce26fface509e0b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff45fd11df734c368b51b22434caa12b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f896a1b3c4bd454d9beafc0535ded9ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 8812\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2755\n",
      "  Number of trainable parameters = 11685891\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2755' max='2755' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2755/2755 40:43, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.036100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.024200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.025700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.027600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/albert-base-v2/checkpoint-500\n",
      "Configuration saved in ./results/albert-base-v2/checkpoint-500/config.json\n",
      "Model weights saved in ./results/albert-base-v2/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to ./results/albert-base-v2/checkpoint-1000\n",
      "Configuration saved in ./results/albert-base-v2/checkpoint-1000/config.json\n",
      "Model weights saved in ./results/albert-base-v2/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to ./results/albert-base-v2/checkpoint-1500\n",
      "Configuration saved in ./results/albert-base-v2/checkpoint-1500/config.json\n",
      "Model weights saved in ./results/albert-base-v2/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to ./results/albert-base-v2/checkpoint-2000\n",
      "Configuration saved in ./results/albert-base-v2/checkpoint-2000/config.json\n",
      "Model weights saved in ./results/albert-base-v2/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to ./results/albert-base-v2/checkpoint-2500\n",
      "Configuration saved in ./results/albert-base-v2/checkpoint-2500/config.json\n",
      "Model weights saved in ./results/albert-base-v2/checkpoint-2500/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 980\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'0': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 185}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 288}, '2': {'precision': 0.5173469387755102, 'recall': 1.0, 'f1-score': 0.6819098856758574, 'support': 507}, 'accuracy': 0.5173469387755102, 'macro avg': {'precision': 0.17244897959183672, 'recall': 0.3333333333333333, 'f1-score': 0.2273032952252858, 'support': 980}, 'weighted avg': {'precision': 0.2676478550603915, 'recall': 0.5173469387755102, 'f1-score': 0.35278399187516296, 'support': 980}}\" of type <class 'dict'> for key \"eval/full_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to albert-base-v2_model\n",
      "Configuration saved in albert-base-v2_model/config.json\n",
      "Model weights saved in albert-base-v2_model/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0155943632125854, 'eval_accuracy': 0.5173469387755102, 'eval_f1': 0.35278399187516296, 'eval_precision': 0.2676478550603915, 'eval_recall': 0.5173469387755102, 'eval_full_report': {'0': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 185}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 288}, '2': {'precision': 0.5173469387755102, 'recall': 1.0, 'f1-score': 0.6819098856758574, 'support': 507}, 'accuracy': 0.5173469387755102, 'macro avg': {'precision': 0.17244897959183672, 'recall': 0.3333333333333333, 'f1-score': 0.2273032952252858, 'support': 980}, 'weighted avg': {'precision': 0.2676478550603915, 'recall': 0.5173469387755102, 'f1-score': 0.35278399187516296, 'support': 980}}, 'eval_runtime': 19.0104, 'eval_samples_per_second': 51.551, 'eval_steps_per_second': 0.842, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on a dataset with 3 labels\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08f2c4b01e0f4d63ac6c8c5b17d7a6ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8d4b4539684ba88bdeb27399847ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/vocab.json\n",
      "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "635fcf8397e74e8da9a19ce725694d4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9937fb9df85f4885acab14f216d0a372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/pytorch_model.bin\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be8617214c041288266084e1bb923b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e14410ef84b944aba978baaf4183dec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 8812\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2755\n",
      "  Number of trainable parameters = 124647939\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2755' max='2755' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2755/2755 39:11, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.027200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.023800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.020400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.019200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.019100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/roberta-base/checkpoint-500\n",
      "Configuration saved in ./results/roberta-base/checkpoint-500/config.json\n",
      "Model weights saved in ./results/roberta-base/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to ./results/roberta-base/checkpoint-1000\n",
      "Configuration saved in ./results/roberta-base/checkpoint-1000/config.json\n",
      "Model weights saved in ./results/roberta-base/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to ./results/roberta-base/checkpoint-1500\n",
      "Configuration saved in ./results/roberta-base/checkpoint-1500/config.json\n",
      "Model weights saved in ./results/roberta-base/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to ./results/roberta-base/checkpoint-2000\n",
      "Configuration saved in ./results/roberta-base/checkpoint-2000/config.json\n",
      "Model weights saved in ./results/roberta-base/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to ./results/roberta-base/checkpoint-2500\n",
      "Configuration saved in ./results/roberta-base/checkpoint-2500/config.json\n",
      "Model weights saved in ./results/roberta-base/checkpoint-2500/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 980\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'0': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 185}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 288}, '2': {'precision': 0.5173469387755102, 'recall': 1.0, 'f1-score': 0.6819098856758574, 'support': 507}, 'accuracy': 0.5173469387755102, 'macro avg': {'precision': 0.17244897959183672, 'recall': 0.3333333333333333, 'f1-score': 0.2273032952252858, 'support': 980}, 'weighted avg': {'precision': 0.2676478550603915, 'recall': 0.5173469387755102, 'f1-score': 0.35278399187516296, 'support': 980}}\" of type <class 'dict'> for key \"eval/full_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Saving model checkpoint to roberta-base_model\n",
      "Configuration saved in roberta-base_model/config.json\n",
      "Model weights saved in roberta-base_model/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0161653757095337, 'eval_accuracy': 0.5173469387755102, 'eval_f1': 0.35278399187516296, 'eval_precision': 0.2676478550603915, 'eval_recall': 0.5173469387755102, 'eval_full_report': {'0': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 185}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 288}, '2': {'precision': 0.5173469387755102, 'recall': 1.0, 'f1-score': 0.6819098856758574, 'support': 507}, 'accuracy': 0.5173469387755102, 'macro avg': {'precision': 0.17244897959183672, 'recall': 0.3333333333333333, 'f1-score': 0.2273032952252858, 'support': 980}, 'weighted avg': {'precision': 0.2676478550603915, 'recall': 0.5173469387755102, 'f1-score': 0.35278399187516296, 'support': 980}}, 'eval_runtime': 16.7579, 'eval_samples_per_second': 58.48, 'eval_steps_per_second': 0.955, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on a dataset with 3 labels\n",
      "ERROR\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "best parameters are:\n",
      "{'clf__alpha': 0.001, 'clf__penalty': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}\n",
      "{'eval_loss': 0.33163265306122447, 'eval_accuracy': 0.6683673469387755, 'eval_f1': 0.5517133704488838, 'eval_precision': 0.7967486967539573, 'eval_recall': 0.5353936036628345, 'eval_full_report': '              precision    recall  f1-score   support\\n\\n           0       0.92      0.18      0.30       185\\n           1       0.85      0.47      0.60       288\\n           2       0.62      0.96      0.75       507\\n\\n    accuracy                           0.67       980\\n   macro avg       0.80      0.54      0.55       980\\nweighted avg       0.74      0.67      0.62       980\\n'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on a dataset with 3 labels\n",
      "ERROR\n",
      "best parameters are:\n",
      "{'clf__alpha': 0.01, 'clf__fit_prior': True, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}\n",
      "{'eval_loss': 0.2510204081632653, 'eval_accuracy': 0.7489795918367347, 'eval_f1': 0.7302273673540053, 'eval_precision': 0.7406893056889793, 'eval_recall': 0.7221644721644721, 'eval_full_report': '              precision    recall  f1-score   support\\n\\n           0       0.71      0.62      0.66       185\\n           1       0.75      0.75      0.75       288\\n           2       0.76      0.79      0.78       507\\n\\n    accuracy                           0.75       980\\n   macro avg       0.74      0.72      0.73       980\\nweighted avg       0.75      0.75      0.75       980\\n'}\n",
      "                    Model            loss        accuracy              f1       precision          recall\n",
      "        bert-base-uncased         0.89947         0.71837         0.71734         0.71715         0.71837\n",
      "           albert-base-v2          1.0156         0.51735         0.35278         0.26765         0.51735\n",
      "             roberta-base          1.0162         0.51735         0.35278         0.26765         0.51735\n",
      "               linear_SVM         0.33163         0.66837         0.55171         0.79675         0.53539\n",
      "multinomial_naive_bayesian         0.25102         0.74898         0.73023         0.74069         0.72216\n"
     ]
    }
   ],
   "source": [
    "from lazytextpredict import basic_classification\n",
    "\n",
    "trial=basic_classification.LTP(Xdata=list(df[\"Text\"].values),Ydata=list(df[\"class\"].values), x_col='Text', y_col='class', models='all') \n",
    "# Xdata is a list of text entries, and Ydata is a list of corresponding labels.\n",
    "# csv and xlsx give options to load data from those file formats (you can pass the file or the file's location)\n",
    "# x_col and y_col are strings that specify the columns of the # text and label columns in your csv or xlsx file respectively.\n",
    "# You can choose between 'transformers'-based, 'count-vectorizer'-based, and 'all' models.\n",
    "\n",
    "trial.run(training_epochs=5) \n",
    "#This trains the models specified above on the data you loaded. \n",
    "#Here you can specify the number of training epochs. \n",
    "#Fewer training epochs will give poorer performance, but will run quicker to allow debugging.\n",
    "\n",
    "trial.print_metrics_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
